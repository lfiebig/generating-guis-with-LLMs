{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pandas as pd\n",
    "from langchain.chains import LLMChain\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "handler = CallbackHandler(os.environ.get(\"LANGFUSE_PUBLIC_KEY\"), os.environ.get(\"LANGFUSE_SECRET_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-4-1106-preview\"\n",
    "MODEL = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "OUTPUT_FOLDER = \"../generated_GUIs/\"\n",
    "IMAGE_INPUT_FOLDER = \"../data/\"\n",
    "S2W = \"../data/s2w_sample_improved.csv\"\n",
    "S2W_SUMMARIZED = \"../data/s2w_summarized.csv\"\n",
    "S2W_SAMPLE = \"../data/s2w_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w = pd.read_csv(S2W)\n",
    "s2w.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "s2w_summarized = pd.read_csv(S2W_SUMMARIZED)\n",
    "s2w_summarized.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "s2w_sample = pd.read_csv(S2W_SAMPLE)\n",
    "s2w_sample.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.prompts import CREATION_PROMPT_FS, CREATION_PROMPT_ZS, CREATION_PROMPT_COT, CREATION_PROMPT_FS_LLM, CREATION_PROMPT_ZS_LLM, CREATION_PROMPT_COT_LLM\n",
    "\n",
    "def create_GUI(id, prompt_type=\"\", input_type=\"\", temp=1):\n",
    "    \n",
    "\n",
    "    summary = s2w_summarized[s2w_summarized[\"screenId\"]==id][input_type].iloc[0]\n",
    "\n",
    "    if MODEL == \"gpt-3.5-turbo-instruct\":\n",
    "        OUTPUT_FOLDER = \"../generated_GUIs/gpt-3.5-turbo-instruct/\"\n",
    "        llm = OpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "\n",
    "    elif MODEL == \"gpt-4-1106-preview\":\n",
    "        OUTPUT_FOLDER = \"../generated_GUIs/gpt-4-1106-preview/\"\n",
    "        llm = ChatOpenAI(model_name=MODEL,temperature=temp)\n",
    "\n",
    "    else:\n",
    "        return \"ERROR: Unknown model provided!\"\n",
    "\n",
    "\n",
    "    if prompt_type == \"ZS\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_ZS, callbacks=[handler])\n",
    "    elif prompt_type == \"FS\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_FS, callbacks=[handler])\n",
    "    elif prompt_type == \"COT\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_COT, callbacks=[handler])\n",
    "    elif prompt_type == \"COT_LLM\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_COT_LLM, callbacks=[handler])\n",
    "    elif prompt_type == \"ZS_LLM\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_ZS_LLM, callbacks=[handler])\n",
    "    elif prompt_type == \"FS_LLM\":\n",
    "        chain = LLMChain(llm=llm, prompt=CREATION_PROMPT_FS_LLM, callbacks=[handler])\n",
    "    else:\n",
    "        return \"ERROR: Unknown prompt type provided!\"\n",
    "\n",
    "    try: \n",
    "        response = chain.run(summary=summary, callbacks=[handler])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    if prompt_type == \"ZS\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/Zero_shot/{input_type}/{id}.html\"\n",
    "    elif prompt_type == \"FS\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/Few_shot/{input_type}/{id}.html\"\n",
    "    elif prompt_type == \"ZS_LLM\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/Zero_shot/llm_summarized/{id}.html\"\n",
    "    elif prompt_type == \"FS_LLM\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/Few_shot/llm_summarized/{id}.html\"\n",
    "    elif prompt_type == \"COT\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/COT/{input_type}/{id}.html\"\n",
    "    elif prompt_type == \"COT_LLM\":\n",
    "        output_file = OUTPUT_FOLDER + f\"/COT/llm_summarized/{id}.html\"\n",
    "    \n",
    "    try: \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(f\"{id} Failed\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creation_pipeline(prompt_type, input_type):\n",
    "    screens = s2w_sample[\"screenId\"].head(10).values\n",
    "    for screen_id in screens:\n",
    "        create_GUI(screen_id, prompt_type=prompt_type, input_type=input_type)\n",
    "        print(f\"{screen_id}: creation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33: creation done\n",
      "70: creation done\n",
      "300: creation done\n",
      "486: creation done\n",
      "494: creation done\n",
      "495: creation done\n",
      "498: creation done\n",
      "549 Failed\n",
      "'charmap' codec can't encode character '\\u25b4' in position 2725: character maps to <undefined>\n",
      "549: creation done\n",
      "596: creation done\n",
      "761: creation done\n"
     ]
    }
   ],
   "source": [
    "creation_pipeline(\"FS\", \"s2w_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
