{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pandas as pd\n",
    "from langchain.chains import LLMChain\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "handler = CallbackHandler(os.environ.get(\"LANGFUSE_PUBLIC_KEY\"), os.environ.get(\"LANGFUSE_SECRET_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "OUTPUT_FOLDER = \"../generated_GUIs/\"\n",
    "IMAGE_INPUT_FOLDER = \"../data/\"\n",
    "S2W = \"../data/Screen2Words.csv\"\n",
    "S2W_SUMMARIZED = \"../data/s2w_summarized.csv\"\n",
    "S2W_SAMPLE = \"../data/s2w_sample.csv\"\n",
    "METADATA = \"../data/metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w = pd.read_csv(S2W)\n",
    "s2w.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "s2w_summarized = pd.read_csv(S2W_SUMMARIZED)\n",
    "s2w_summarized.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "s2w_sample = pd.read_csv(S2W_SAMPLE)\n",
    "s2w_sample.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "rico = pd.read_csv(METADATA)\n",
    "rico.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import SUMMARY2REQUIREMENTS_ZS, REQUIREMENTS2IMPLEMENTATION_ZS, IMPLEMETATION2STRUCTURE_ZS, STRUCTURE2CODE_ZS\n",
    "\n",
    "def s2r2e2s2c (summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain_1 = LLMChain(llm=llm, prompt=SUMMARY2REQUIREMENTS_ZS, output_key=\"requirements\")\n",
    "    chain_2 = LLMChain(llm=llm, prompt=REQUIREMENTS2IMPLEMENTATION_ZS, output_key=\"implementation\")\n",
    "    chain_3 = LLMChain(llm=llm, prompt=IMPLEMETATION2STRUCTURE_ZS, output_key=\"structure\")\n",
    "    chain_4 = LLMChain(llm=llm, prompt=STRUCTURE2CODE_ZS, output_key=\"code\")\n",
    "\n",
    "    seq_chain = SequentialChain(\n",
    "        chains = [chain_1,chain_2,chain_3,chain_4],\n",
    "        input_variables=[\"summary\"])\n",
    "    \n",
    "    return(seq_chain.run(summary, callbacks=[handler]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import SUMMARY2REQUIRMENTS_FS, REQUIREMENTS2IMPLEMENTATION_FS, IMPLEMETATION2STRUCTURE_FS, STRUCTURE2CODE_FS\n",
    "\n",
    "def s2r2e2s2c_fs (summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain_1 = LLMChain(llm=llm, prompt=SUMMARY2REQUIRMENTS_FS, output_key=\"requirements\")\n",
    "    chain_2 = LLMChain(llm=llm, prompt=REQUIREMENTS2IMPLEMENTATION_FS, output_key=\"implementation\")\n",
    "    chain_3 = LLMChain(llm=llm, prompt=IMPLEMETATION2STRUCTURE_FS, output_key=\"structure\")\n",
    "    chain_4 = LLMChain(llm=llm, prompt=STRUCTURE2CODE_FS, output_key=\"code\")\n",
    "\n",
    "    seq_chain = SequentialChain(\n",
    "        chains = [chain_1,chain_2,chain_3,chain_4],\n",
    "        input_variables=[\"summary\"])\n",
    "    \n",
    "    return(seq_chain.run(summary, callbacks=[handler]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import SUMMARY2STRUCTURE_ZS, STRUCTURE2CODE_ZS\n",
    "\n",
    "def s2s2c (summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain_1 = LLMChain(llm=llm, prompt=SUMMARY2STRUCTURE_ZS, output_key=\"structure\",)\n",
    "    chain_2 = LLMChain(llm=llm, prompt=STRUCTURE2CODE_ZS, output_key=\"code\",)\n",
    "\n",
    "    seq_chain = SequentialChain(\n",
    "        chains = [chain_1,chain_2],\n",
    "        input_variables=[\"summary\"])\n",
    "    \n",
    "    return(seq_chain.run(summary, callbacks=[handler]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import SUMMARY2STRUCTURE_FS, STRUCTURE2CODE_FS\n",
    "\n",
    "def s2s2c_fs (summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain_1 = LLMChain(llm=llm, prompt=SUMMARY2STRUCTURE_FS, output_key=\"structure\",)\n",
    "    chain_2 = LLMChain(llm=llm, prompt=STRUCTURE2CODE_FS, output_key=\"code\",)\n",
    "\n",
    "    seq_chain = SequentialChain(\n",
    "        chains = [chain_1,chain_2],\n",
    "        input_variables=[\"summary\"])\n",
    "    \n",
    "    return(seq_chain.run(summary, callbacks=[handler]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import SUMMARY2CODE_ZS\n",
    "\n",
    "def s2c(summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain = LLMChain(llm=llm, prompt=SUMMARY2CODE_ZS, callbacks=[handler])\n",
    "\n",
    "    try: \n",
    "        response = chain.run(summary=summary, callbacks=[handler])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ALLINONEPROMPT_ZS\n",
    "\n",
    "def s2c_xot_zs(summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain = LLMChain(llm=llm, prompt=ALLINONEPROMPT_ZS, callbacks=[handler])\n",
    "\n",
    "    try: \n",
    "        response = chain.run(summary=summary, callbacks=[handler])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import ALLINONEPROMPT_FS\n",
    "\n",
    "def s2c_xot_fs(summary, temp=0.7):\n",
    "    llm = ChatOpenAI(model_name=MODEL,temperature=temp, max_tokens=1200)\n",
    "    chain = LLMChain(llm=llm, prompt=ALLINONEPROMPT_FS, callbacks=[handler])\n",
    "\n",
    "    try: \n",
    "        response = chain.run(summary=summary, callbacks=[handler])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(summary):\n",
    "    \n",
    "    response = s2c(summary)\n",
    "    try: \n",
    "        with open(\"instruction.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "    \n",
    "    response = s2c_xot_zs(summary)\n",
    "    try: \n",
    "        with open(\"xot_zs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "\n",
    "    response = s2c_xot_fs(summary)\n",
    "    try: \n",
    "        with open(\"xot_fs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "    \n",
    "    response = s2s2c(summary)\n",
    "    try: \n",
    "        with open(\"2_step_zs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "    \n",
    "    response = s2s2c_fs(summary)\n",
    "    try: \n",
    "        with open(\"2_step_fs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "    \n",
    "    response = s2r2e2s2c(summary)\n",
    "    try: \n",
    "        with open(\"question_decomposition_zs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n",
    "    \n",
    "    response = s2r2e2s2c_fs(summary)\n",
    "    try: \n",
    "        with open(\"question_decomposition_fs.html\", 'w') as f:\n",
    "            f.write(response)\n",
    "    except Exception as e:\n",
    "        print(\"Failed\")\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(\"The screen displays a track playing in a music application, along with various options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
